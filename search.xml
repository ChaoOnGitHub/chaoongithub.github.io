<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Python版自然语言处理快速指南]]></title>
    <url>%2F2018%2F05%2F27%2FPython%E7%89%88%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86%E5%BF%AB%E9%80%9F%E6%8C%87%E5%8D%97%2F</url>
    <content type="text"><![CDATA[1. NLP简介 标记化 把文本转化为标记的过程 标记 文本中出现的实体或词汇 文本对象 句子/短语/词汇/文章 2. 文本处理 移除噪声 词汇规范化 对象标准化 2.1 移除噪声 常常将文本中的一些与结果无关的作为噪声。例如停用词、网站链接、特殊符号等等，可以做如下的python实现： 123456noise_word=['a','an','the','of','this','!']def text_remove_noise(text): words=text.split() removed_words=[word for word in words if word not in noise_word] removed_words_string=" ".join(removed_words) return removed_words_string 1text_remove_noise("this is an apple !") 'is apple' 2.2 词汇规范化 词干提取 词形还原 12345678from nltk.stem.wordnet import WordNetLemmatizerfrom nltk.stem.porter import PorterStemmerlem=WordNetLemmatizer()stem=PorterStemmer()word1="doing"lem.lemmatize(word1,"v") u'do' 12word2="multiply"stem.stem(word2) u'multipli' 2.3 对象标准化 主要是将文本中在词典中不出现的词进行标准化 1234567891011lookup_dict=&#123;'rt':'Retweet','dm':'direct message','awsm':'awesome','luv':'love'&#125;def lookup_words(text): words=text.split() new_words=[] for word in words: if word.lower() in lookup_dict: word=lookup_dict[word.lower()] new_words.append(word) new_text=" ".join(new_words) return new_text 1lookup_words("RT is an awsm man") 'Retweet is an awesome man' 3.文本到特征 其实就是从文本中进行特征工程，主要包括句法分析（分词、词性标注、依存句法分析）、实体/N元模型、统计特征和词嵌入 。 3.1 依存句法分析 主要是以谓词为中心而构句，词与词之间的关系，用一种有向图来表示。 常用的依存句法分析，有Stanford Parser。 词性标注 词性标注用途有消除歧义、一词多性、不同单词的特征、词性还原、更有效的移除停用词。 1234from nltk import word_tokenize,pos_tagtext="I am learning Natural Language Processing on Analytics Vidhya"tokens=word_tokenize(text)print tokens ['I', 'am', 'learning', 'Natural', 'Language', 'Processing', 'on', 'Analytics', 'Vidhya'] 1print pos_tag(tokens) [('I', 'PRP'), ('am', 'VBP'), ('learning', 'VBG'), ('Natural', 'NNP'), ('Language', 'NNP'), ('Processing', 'NNP'), ('on', 'IN'), ('Analytics', 'NNP'), ('Vidhya', 'NNP')] 3.2 实体提取（实体作为特征） 实体主要是指句子中的名称或者动词组成一个实体词。实体识别通常要综合词典、位置标注和依存句法分析进行分析。目前主要分为主题模型和命名实体识别。 命名实体识别 NER通常包括三个模块：名词短语识别：根据从属关系分析和词性标注提取文本中所有的名称短语。短语分类：把提取出来的名词短语分到相应的类别。实体消除歧义：被错误分类的结果进行一层的检验，知识图谱主要包括Google知识图谱，IBM Watson和Wikipedia。 主题模型 主题模型主要是识别文本所属的主题，会将文本中分成若干主题，主题内部的词会比较相似。目前主流的主题建模技术是潜在狄利克雷模型LDA 123# LDA模型import gensim from gensim import corpora 123456doc1 = "Sugar is bad to consume. My sister likes to have sugar, but not my father."doc2 = "My father spends a lot of time driving my sister around to dance practice."doc3 = "Doctors suggest that driving may cause increased stress and blood pressure."doc_list=[doc1,doc2,doc3]doc_clean=[doc.split() for doc in doc_list] 1234567891011# 建立语料库词典dictionary=corpora.Dictionary(doc_clean)# 构建文档-词 的矩阵doc_term_matrix=[dictionary.doc2bow(doc) for doc in doc_clean]# 使用gensim构建LDA模型Lda=gensim.models.ldamodel.LdaModel# 训练LDA模型ldamodel=Lda(doc_term_matrix,num_topics=3,id2word=dictionary,passes=50) 1print ldamodel.print_topics() [(0, u'0.053*&quot;driving&quot; + 0.053*&quot;My&quot; + 0.053*&quot;my&quot; + 0.053*&quot;sister&quot; + 0.053*&quot;to&quot; + 0.053*&quot;practice.&quot; + 0.053*&quot;of&quot; + 0.053*&quot;a&quot; + 0.053*&quot;father&quot; + 0.053*&quot;around&quot;'), (1, u'0.029*&quot;driving&quot; + 0.029*&quot;sister&quot; + 0.029*&quot;My&quot; + 0.029*&quot;my&quot; + 0.029*&quot;to&quot; + 0.029*&quot;may&quot; + 0.029*&quot;and&quot; + 0.029*&quot;Doctors&quot; + 0.029*&quot;stress&quot; + 0.029*&quot;blood&quot;'), (2, u'0.063*&quot;to&quot; + 0.036*&quot;have&quot; + 0.036*&quot;father.&quot; + 0.036*&quot;but&quot; + 0.036*&quot;not&quot; + 0.036*&quot;consume.&quot; + 0.036*&quot;Sugar&quot; + 0.036*&quot;likes&quot; + 0.036*&quot;is&quot; + 0.036*&quot;bad&quot;')] 把N元文法作为特征 N元文法通常比单词包含更多信息，也是文本的重要特征 123456def generate_ngrams(text,n): words=text.split() output=[] for i in range(len(words)-n+1): output.append(words[i:i+n]) return output 1generate_ngrams('I am learning Natural Language Processing on Analytics Vidhya',2) [['I', 'am'], ['am', 'learning'], ['learning', 'Natural'], ['Natural', 'Language'], ['Language', 'Processing'], ['Processing', 'on'], ['on', 'Analytics'], ['Analytics', 'Vidhya']] 3.3 统计特征 TF-IDF 1from sklearn.feature_extraction.text import TfidfVectorizer 1234tfidf=TfidfVectorizer()corpus=['This is sample document.', 'another random document.', 'third sample document text']X=tfidf.fit_transform(corpus)print X (0, 7) 0.58448290102 (0, 2) 0.58448290102 (0, 4) 0.444514311537 (0, 1) 0.345205016865 (1, 1) 0.385371627466 (1, 0) 0.652490884513 (1, 3) 0.652490884513 (2, 4) 0.444514311537 (2, 1) 0.345205016865 (2, 6) 0.58448290102 (2, 5) 0.58448290102 3.4 词嵌入 词嵌入通常是把高维向量转化为低维向量。通常是做深度学习的基础。目前主要用的有Word2Vec和GloVe。 123456from gensim.models import Word2Vecsentences=[['data', 'science'], ['vidhya', 'science', 'data', 'analytics'],['machine', 'learning'], ['deep', 'learning']]model=Word2Vec(sentences,min_count=1)print model.similarity('data','science') -0.236613294643 4 NLP重要任务 4.1 文本分类 文本分类是NLP主要任务，主要是将文本分成不同的类别，主要包括训练分类器和预测。 12345678910111213141516171819training_corpus = [ ('I am exhausted of this work.', 'Class_B'), ("I can't cooperate with this", 'Class_B'), ('He is my badest enemy!', 'Class_B'), ('My management is poor.', 'Class_B'), ('I love this burger.', 'Class_A'), ('This is an brilliant place!', 'Class_A'), ('I feel very good about these dates.', 'Class_A'), ('This is my best work.', 'Class_A'), ("What an awesome view", 'Class_A'), ('I do not like this dish', 'Class_B')]test_corpus = [ ("I am not feeling well today.", 'Class_B'), ("I feel brilliant!", 'Class_A'), ('Gary is a friend of mine.', 'Class_A'), ("I can't believe I'm doing this.", 'Class_B'), ('The date was good.', 'Class_A'), ('I do not enjoy my job', 'Class_B')] 1234567891011121314151617181920212223242526from sklearn.feature_extraction.text import TfidfVectorizerfrom sklearn.metrics import classification_reportfrom sklearn import svmtrain_data=[]train_labels=[]for row in training_corpus: train_data.append(row[0]) train_labels.append(row[1])test_data=[]test_labels=[]for row in test_corpus: test_data.append(row[0]) test_labels.append(row[1]) vectorizer=TfidfVectorizer(min_df=4,max_df=0.9)train_vectors=vectorizer.fit_transform(train_data)test_vectors=vectorizer.transform(test_data)model=svm.SVC(kernel='linear')model.fit(train_vectors,train_labels)prediction=model.predict(test_vectors)print prediction ['Class_A' 'Class_A' 'Class_B' 'Class_B' 'Class_A' 'Class_A'] 1print classification_report(test_labels,prediction) precision recall f1-score support Class_A 0.50 0.67 0.57 3 Class_B 0.50 0.33 0.40 3 avg / total 0.50 0.50 0.49 6 4.2 文本匹配度/相似度 计算文本匹配度也是NLP的另一个重要任务 编辑距离 Levenshtein 12345678910111213def levenshtein(s1,s2): if len(s1) &gt; len(s2): s1,s2 = s2,s1 distances = range(len(s1) + 1) for index2,char2 in enumerate(s2): newDistances = [index2+1] for index1,char1 in enumerate(s1): if char1 == char2: newDistances.append(distances[index1]) else: newDistances.append(1 + min((distances[index1], distances[index1+1], newDistances[-1]))) distances = newDistances return distances[-1] 1print(levenshtein("anew","ans")) 2 余弦相似度 12import mathfrom collections import Counter 1234567891011121314151617181920212223def cosine(vec1,vec2): common=set(vec1.keys())&amp;set(vec2.keys()) numerator=sum([vec1[x]*vec2[x] for x in common]) sum1=sum([vec1[x]**2 for x in vec1.keys()]) sum2=sum([vec2[x]**2 for x in vec2.keys()]) denominator=math.sqrt(sum1)*math.sqrt(sum2) if not denominator: return 0.0 else: return float(numerator)/denominatordef text_to_vector(text): words=text.split() return Counter(words)text1 = 'This is an article on analytics vidhya'text2 = 'article on analytics vidhya is about natural language processing'vector1 = text_to_vector(text1)vector2 = text_to_vector(text2)cosine = cosine(vector1, vector2)print cosine 0.629940788349 5. NLP其他任务 文本自动摘要 机器翻译 文档自动生成 文本信息抽取 6. 主要的NLP库 scikit-learn NLTK Pattern 网页挖掘模块 可以和NLTK搭配使用 TextBlob 基于NLTK和Pattern架构的NLP spacy 工业级的NLP库 gensim word2vec构建、文本抽取 Stanford CoreNLP 12]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>Python</tag>
        <tag>快速指南</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用LDA模型判断技术主题分布]]></title>
    <url>%2F2018%2F04%2F21%2F%E5%88%A9%E7%94%A8LDA%E6%A8%A1%E5%9E%8B%E5%88%A4%E6%96%AD%E6%8A%80%E6%9C%AF%E4%B8%BB%E9%A2%98%E5%88%86%E5%B8%83%2F</url>
    <content type="text"><![CDATA[0. 导入相关包 1234567from pandas import DataFrame,Seriesimport pandas as pdimport numpy as npimport osimport reimport gensimfrom gensim import corpora,similarities,models 1. 读入数据 1234567891011121314os.chdir('/Users/zhanghc/learning/for3d/转完')filelist=[x for x in os.listdir('.') if os.path.isfile(x) and os.path.splitext(x)[1]=='.txt']i=0dict_data=&#123;&#125;for filetxt in filelist: i+=1 print("开始读入第%d篇文档: %s" % (i,filetxt)) if i &lt;= len(filelist): raw_data=pd.read_table(os.path.abspath(filetxt),sep="\t",index_col=None) dict_data.update(raw_data.iloc[:,4].to_dict()) else: breakprint len(dict_data) 开始读入第1篇文档 开始读入第2篇文档 开始读入第3篇文档 开始读入第4篇文档 开始读入第5篇文档 开始读入第6篇文档 开始读入第7篇文档 开始读入第8篇文档 开始读入第9篇文档 开始读入第10篇文档 开始读入第11篇文档 开始读入第12篇文档 开始读入第13篇文档 开始读入第14篇文档 开始读入第15篇文档 开始读入第16篇文档 开始读入第17篇文档 开始读入第18篇文档 开始读入第19篇文档 开始读入第20篇文档 开始读入第21篇文档 开始读入第22篇文档 开始读入第23篇文档 开始读入第24篇文档 开始读入第25篇文档 开始读入第26篇文档 开始读入第27篇文档 开始读入第28篇文档 开始读入第29篇文档 开始读入第30篇文档 开始读入第31篇文档 开始读入第32篇文档 开始读入第33篇文档 开始读入第34篇文档 开始读入第35篇文档 开始读入第36篇文档 开始读入第37篇文档 开始读入第38篇文档 开始读入第39篇文档 开始读入第40篇文档 开始读入第41篇文档 开始读入第42篇文档 开始读入第43篇文档 开始读入第44篇文档 开始读入第45篇文档 开始读入第46篇文档 开始读入第47篇文档 开始读入第48篇文档 开始读入第49篇文档 开始读入第50篇文档 开始读入第51篇文档 开始读入第52篇文档 开始读入第53篇文档 开始读入第54篇文档 开始读入第55篇文档 开始读入第56篇文档 开始读入第57篇文档 开始读入第58篇文档 开始读入第59篇文档 开始读入第60篇文档 开始读入第61篇文档 开始读入第62篇文档 开始读入第63篇文档 开始读入第64篇文档 开始读入第65篇文档 开始读入第66篇文档 开始读入第67篇文档 开始读入第68篇文档 开始读入第69篇文档 开始读入第70篇文档 开始读入第71篇文档 开始读入第72篇文档 开始读入第73篇文档 开始读入第74篇文档 开始读入第75篇文档 开始读入第76篇文档 开始读入第77篇文档 开始读入第78篇文档 开始读入第79篇文档 开始读入第80篇文档 开始读入第81篇文档 开始读入第82篇文档 开始读入第83篇文档 开始读入第84篇文档 开始读入第85篇文档 开始读入第86篇文档 开始读入第87篇文档 开始读入第88篇文档 开始读入第89篇文档 开始读入第90篇文档 开始读入第91篇文档 开始读入第92篇文档 开始读入第93篇文档 开始读入第94篇文档 开始读入第95篇文档 开始读入第96篇文档 开始读入第97篇文档 开始读入第98篇文档 开始读入第99篇文档 开始读入第100篇文档 开始读入第101篇文档 开始读入第102篇文档 开始读入第103篇文档 开始读入第104篇文档 开始读入第105篇文档 开始读入第106篇文档 开始读入第107篇文档 开始读入第108篇文档 开始读入第109篇文档 开始读入第110篇文档 开始读入第111篇文档 开始读入第112篇文档 开始读入第113篇文档 开始读入第114篇文档 开始读入第115篇文档 开始读入第116篇文档 开始读入第117篇文档 开始读入第118篇文档 开始读入第119篇文档 开始读入第120篇文档 开始读入第121篇文档 开始读入第122篇文档 开始读入第123篇文档 开始读入第124篇文档 开始读入第125篇文档 开始读入第126篇文档 开始读入第127篇文档 开始读入第128篇文档 开始读入第129篇文档 开始读入第130篇文档 开始读入第131篇文档 开始读入第132篇文档 开始读入第133篇文档 开始读入第134篇文档 开始读入第135篇文档 开始读入第136篇文档 开始读入第137篇文档 开始读入第138篇文档 开始读入第139篇文档 开始读入第140篇文档 开始读入第141篇文档 开始读入第142篇文档 开始读入第143篇文档 开始读入第144篇文档 开始读入第145篇文档 开始读入第146篇文档 开始读入第147篇文档 开始读入第148篇文档 开始读入第149篇文档 开始读入第150篇文档 开始读入第151篇文档 开始读入第152篇文档 开始读入第153篇文档 开始读入第154篇文档 开始读入第155篇文档 开始读入第156篇文档 开始读入第157篇文档 开始读入第158篇文档 开始读入第159篇文档 开始读入第160篇文档 开始读入第161篇文档 开始读入第162篇文档 开始读入第163篇文档 开始读入第164篇文档 开始读入第165篇文档 开始读入第166篇文档 开始读入第167篇文档 开始读入第168篇文档 开始读入第169篇文档 开始读入第170篇文档 开始读入第171篇文档 开始读入第172篇文档 开始读入第173篇文档 开始读入第174篇文档 开始读入第175篇文档 开始读入第176篇文档 开始读入第177篇文档 开始读入第178篇文档 开始读入第179篇文档 开始读入第180篇文档 开始读入第181篇文档 开始读入第182篇文档 开始读入第183篇文档 开始读入第184篇文档 开始读入第185篇文档 开始读入第186篇文档 开始读入第187篇文档 开始读入第188篇文档 开始读入第189篇文档 开始读入第190篇文档 开始读入第191篇文档 开始读入第192篇文档 开始读入第193篇文档 开始读入第194篇文档 开始读入第195篇文档 开始读入第196篇文档 开始读入第197篇文档 开始读入第198篇文档 开始读入第199篇文档 开始读入第200篇文档 9123 2. 构建语料 123456789101112## 文本预处理def text_clean(text): text=text.replace("\n"," ") text=text.replace("-"," ") text=text.replace("("," ") text=text.replace(")"," ") pure_text='' for letter in text: if letter.isalpha() or letter==" ": pure_text+=letter text=' '.join(word for word in pure_text.split() if len(word)&gt;1) return text 12345678## 构建停用词表stoplist=[]with open('/Users/zhanghc/learning/for3d/stop_words_eng.txt') as f: for line in f.readlines(): line=line.replace('\r','') line=line.replace('\n','') if line not in stoplist: stoplist.append(line) 1234## 创建语料dict_values=dict_data.values()## 去除停用词patent_text=[[word.lower() for word in text_clean(doc).split() if word.lower() not in stoplist] for doc in dict_values ] 1patent_text[0] ['hydroxyethylpyrrolidone', 'methacrylateglycidyl', 'methacrylate', 'copolymer', 'prepared', 'precipitation', 'polymerization', 'hydroxyethylpyrrolidone', 'methacrylateglycidyl', 'methacrylate', 'copolymer', 'composition', 'adhesive', 'aerosol', 'agricultural', 'agent', 'anti', 'soil', 'redeposition', 'agent', 'battery', 'agent', 'beverage', 'biocide', 'cementing', 'construction', 'agent', 'cleaning', 'agent', 'coating', 'agent', 'conductive', 'material', 'cosmetic', 'agent', 'crosslinking', 'agent', 'dental', 'agent', 'decorated', 'pigment', 'delivery', 'active', 'agent', 'detergent', 'dispersant', 'drug', 'electronic', 'encapsulation', 'food', 'hair', 'spray', 'domestic', 'industrial', 'institutional', 'ink', 'coating', 'interlaminate', 'adhesive', 'lithographic', 'solution', 'membrane', 'additive', 'agent', 'metal', 'working', 'fluid', 'oilfield', 'agent', 'paint', 'paper', 'paper', 'sizing', 'agent', 'personal', 'care', 'agent', 'pharmaceutical', 'pigment', 'additive', 'plaster', 'plastic', 'printing', 'refractive', 'index', 'modifier', 'rheological', 'modifier', 'reactive', 'polymer', 'agent', 'sequestrant', 'soil', 'release', 'agent', 'static', 'control', 'agent', 'surface', 'modification', 'agent', 'thickener', 'wood', 'care', 'agent', 'claimed', 'reactive', 'flexible', 'water', 'resistant', 'hydroxyethylpyrrolidone', 'methacrylateglycidyl', 'methacrylate', 'copolymer', 'manufactured', 'easy', 'method', 'yield', 'detailed', 'description', 'hydroxyethylpyrrolidone', 'methacrylateglycidyl', 'methacrylate', 'copolymer', 'formula', 'prepared', 'precipitation', 'polymerization', 'mol', 'moland', 'xy', 'mol', 'independent', 'claims', 'composition', 'comprises', 'copolymer', 'wt', 'additive', 'preparation', 'copolymer'] 12345##建立语料### 创建整个语料的词典dictionary=corpora.Dictionary(patent_text)### 将每个文本转换成向量表示corpus=[(dictionary.doc2bow(text)) for text in patent_text] 3. 建立LDA模型 1lda=gensim.models.ldamodel.LdaModel(corpus=corpus,num_topics=20,id2word=dictionary) 12# 查看一下LDA模型的结果lda.print_topics(num_topics=20,num_words=10) [(0, u'0.036*&quot;light&quot; + 0.033*&quot;device&quot; + 0.027*&quot;image&quot; + 0.018*&quot;drawing&quot; + 0.018*&quot;signal&quot; + 0.017*&quot;digital&quot; + 0.015*&quot;unit&quot; + 0.012*&quot;description&quot; + 0.012*&quot;display&quot; + 0.010*&quot;camera&quot;'), (1, u'0.029*&quot;material&quot; + 0.022*&quot;wt&quot; + 0.020*&quot;resin&quot; + 0.014*&quot;agent&quot; + 0.012*&quot;additive&quot; + 0.011*&quot;comprises&quot; + 0.011*&quot;pts&quot; + 0.008*&quot;powder&quot; + 0.008*&quot;description&quot; + 0.008*&quot;degrees&quot;'), (2, u'0.026*&quot;coating&quot; + 0.025*&quot;composition&quot; + 0.013*&quot;layer&quot; + 0.011*&quot;printing&quot; + 0.010*&quot;film&quot; + 0.010*&quot;ink&quot; + 0.009*&quot;material&quot; + 0.008*&quot;comprising&quot; + 0.008*&quot;surface&quot; + 0.007*&quot;component&quot;'), (3, u'0.060*&quot;layer&quot; + 0.034*&quot;film&quot; + 0.027*&quot;substrate&quot; + 0.015*&quot;electrode&quot; + 0.015*&quot;semiconductor&quot; + 0.011*&quot;drawing&quot; + 0.010*&quot;surface&quot; + 0.009*&quot;formed&quot; + 0.009*&quot;circuit&quot; + 0.009*&quot;description&quot;'), (4, u'0.045*&quot;data&quot; + 0.019*&quot;image&quot; + 0.017*&quot;object&quot; + 0.016*&quot;apparatus&quot; + 0.014*&quot;drawing&quot; + 0.012*&quot;description&quot; + 0.012*&quot;unit&quot; + 0.011*&quot;processing&quot; + 0.011*&quot;device&quot; + 0.010*&quot;method&quot;'), (5, u'0.076*&quot;laser&quot; + 0.020*&quot;beam&quot; + 0.017*&quot;light&quot; + 0.015*&quot;scanning&quot; + 0.012*&quot;wire&quot; + 0.012*&quot;drawing&quot; + 0.011*&quot;description&quot; + 0.010*&quot;device&quot; + 0.010*&quot;welding&quot; + 0.009*&quot;optical&quot;'), (6, u'0.026*&quot;composition&quot; + 0.022*&quot;compound&quot; + 0.021*&quot;polymer&quot; + 0.014*&quot;article&quot; + 0.013*&quot;organic&quot; + 0.011*&quot;group&quot; + 0.010*&quot;alkyl&quot; + 0.009*&quot;comprises&quot; + 0.008*&quot;comprising&quot; + 0.008*&quot;description&quot;'), (7, u'0.077*&quot;wt&quot; + 0.053*&quot;pts&quot; + 0.023*&quot;adding&quot; + 0.022*&quot;water&quot; + 0.014*&quot;degrees&quot; + 0.012*&quot;stirring&quot; + 0.011*&quot;minutes&quot; + 0.011*&quot;mixing&quot; + 0.009*&quot;powder&quot; + 0.009*&quot;comprises&quot;'), (8, u'0.036*&quot;wt&quot; + 0.028*&quot;pts&quot; + 0.011*&quot;fabric&quot; + 0.011*&quot;ink&quot; + 0.011*&quot;ion&quot; + 0.009*&quot;additive&quot; + 0.009*&quot;comprises&quot; + 0.008*&quot;aqueous&quot; + 0.008*&quot;paper&quot; + 0.008*&quot;electrolyte&quot;'), (9, u'0.028*&quot;module&quot; + 0.018*&quot;connected&quot; + 0.016*&quot;control&quot; + 0.014*&quot;drawing&quot; + 0.013*&quot;print&quot; + 0.013*&quot;device&quot; + 0.012*&quot;light&quot; + 0.011*&quot;model&quot; + 0.011*&quot;printing&quot; + 0.010*&quot;sensor&quot;'), (10, u'0.019*&quot;material&quot; + 0.017*&quot;structure&quot; + 0.015*&quot;feeding&quot; + 0.015*&quot;screw&quot; + 0.013*&quot;rod&quot; + 0.011*&quot;device&quot; + 0.011*&quot;shaft&quot; + 0.009*&quot;connected&quot; + 0.009*&quot;inner&quot; + 0.009*&quot;section&quot;'), (11, u'0.038*&quot;powder&quot; + 0.021*&quot;tissue&quot; + 0.013*&quot;food&quot; + 0.012*&quot;method&quot; + 0.011*&quot;parts&quot; + 0.009*&quot;mass&quot; + 0.009*&quot;cells&quot; + 0.008*&quot;material&quot; + 0.007*&quot;bag&quot; + 0.007*&quot;printing&quot;'), (12, u'0.034*&quot;manufacturing&quot; + 0.032*&quot;material&quot; + 0.028*&quot;additive&quot; + 0.019*&quot;method&quot; + 0.016*&quot;layer&quot; + 0.014*&quot;description&quot; + 0.014*&quot;drawing&quot; + 0.014*&quot;surface&quot; + 0.013*&quot;powder&quot; + 0.012*&quot;component&quot;'), (13, u'0.020*&quot;temperature&quot; + 0.019*&quot;device&quot; + 0.015*&quot;air&quot; + 0.011*&quot;plate&quot; + 0.011*&quot;supporting&quot; + 0.011*&quot;support&quot; + 0.010*&quot;control&quot; + 0.010*&quot;heat&quot; + 0.010*&quot;upper&quot; + 0.010*&quot;drawing&quot;'), (14, u'0.052*&quot;nozzle&quot; + 0.018*&quot;printing&quot; + 0.018*&quot;head&quot; + 0.016*&quot;printer&quot; + 0.016*&quot;filament&quot; + 0.012*&quot;material&quot; + 0.011*&quot;pipe&quot; + 0.011*&quot;dimensional&quot; + 0.011*&quot;cooling&quot; + 0.010*&quot;drawing&quot;'), (15, u'0.013*&quot;liquid&quot; + 0.011*&quot;powder&quot; + 0.010*&quot;metal&quot; + 0.010*&quot;water&quot; + 0.010*&quot;method&quot; + 0.010*&quot;tank&quot; + 0.010*&quot;material&quot; + 0.009*&quot;additive&quot; + 0.009*&quot;temperature&quot; + 0.008*&quot;solution&quot;'), (16, u'0.022*&quot;layer&quot; + 0.021*&quot;glass&quot; + 0.015*&quot;material&quot; + 0.011*&quot;fiber&quot; + 0.010*&quot;surface&quot; + 0.010*&quot;film&quot; + 0.010*&quot;sheet&quot; + 0.009*&quot;paper&quot; + 0.009*&quot;plastic&quot; + 0.008*&quot;extrusion&quot;'), (17, u'0.033*&quot;printing&quot; + 0.030*&quot;device&quot; + 0.020*&quot;plate&quot; + 0.015*&quot;printer&quot; + 0.015*&quot;dimensional&quot; + 0.014*&quot;model&quot; + 0.012*&quot;connected&quot; + 0.011*&quot;drawing&quot; + 0.010*&quot;mechanism&quot; + 0.010*&quot;body&quot;'), (18, u'0.023*&quot;module&quot; + 0.020*&quot;signal&quot; + 0.020*&quot;digital&quot; + 0.016*&quot;optical&quot; + 0.013*&quot;drawing&quot; + 0.012*&quot;processing&quot; + 0.012*&quot;light&quot; + 0.011*&quot;method&quot; + 0.011*&quot;connected&quot; + 0.009*&quot;frequency&quot;'), (19, u'0.032*&quot;method&quot; + 0.019*&quot;drawing&quot; + 0.015*&quot;device&quot; + 0.014*&quot;dimensional&quot; + 0.014*&quot;description&quot; + 0.010*&quot;step&quot; + 0.008*&quot;enables&quot; + 0.008*&quot;data&quot; + 0.007*&quot;involves&quot; + 0.006*&quot;process&quot;')] 4.查看一下十项技术的所属于的主题 12tech_text=['selective laser sinter','direct metal laser sinter','fused deposit model','digital light process','laminated object manufact', 'selective heat sinter','electron beam melting','selective heat sintering','powder bed and inkjet head','Polymer injection'] 12345678## 打印10个技术的主题分布情况i=0for text in tech_text: i+=1 text_lower_split=text.lower().split() text_lda=dictionary.doc2bow(document=text_lower_split) topics=lda.get_document_topics(bow=text_lda) print("与第%d技术:%s 相似的主题为:%s" % (i,text,topics)) 与第1技术:selective laser sinter 相似的主题为:[(0, 0.01250000000026427), (1, 0.012500000025155725), (2, 0.012500000000366615), (3, 0.012500000001424455), (4, 0.012500000001358267), (5, 0.76249999979937455), (6, 0.012500000004073491), (7, 0.012500000005187828), (8, 0.012500000000648456), (9, 0.012500000001325307), (10, 0.012500000003560267), (11, 0.012500000011537189), (12, 0.012500000056974625), (13, 0.0125000000046952), (14, 0.012500000004273579), (15, 0.012500000041852151), (16, 0.012500000008832297), (17, 0.012500000020454457), (18, 0.012500000006792982), (19, 0.012500000001848336)] 与第2技术:direct metal laser sinter 相似的主题为:[(0, 0.0100000000176164), (1, 0.01000000001846911), (2, 0.010000000052152326), (3, 0.010000000083898255), (4, 0.010000000012798747), (5, 0.80999999924483801), (6, 0.010000000067836134), (7, 0.010000000000953272), (8, 0.010000000054614001), (9, 0.010000000009113835), (10, 0.010000000008459112), (11, 0.010000000002772447), (12, 0.010000000136079469), (13, 0.01000000003201116), (14, 0.010000000028306415), (15, 0.010000000119506204), (16, 0.010000000029422841), (17, 0.010000000035681762), (18, 0.010000000033563822), (19, 0.010000000011906259)] 与第3技术:fused deposit model 相似的主题为:[(0, 0.012500000018879805), (1, 0.012500000197938626), (2, 0.012500000017006902), (3, 0.012500000090671566), (4, 0.012500000089346331), (5, 0.28533967238960384), (6, 0.012500000085532138), (7, 0.01250000000207324), (8, 0.012500000006303191), (9, 0.012500000455888722), (10, 0.012500000115682725), (11, 0.012500000141893114), (12, 0.012500000533711652), (13, 0.012500000177804612), (14, 0.012500000427409754), (15, 0.012500000038312744), (16, 0.012500000120303622), (17, 0.48966032482276883), (18, 0.012500000068109227), (19, 0.012500000200759476)] 与第4技术:digital light process 相似的主题为:[(0, 0.76249999637024868), (1, 0.012500000109553058), (2, 0.012500000085585416), (3, 0.012500000249875973), (4, 0.012500000217672084), (5, 0.012500000363651726), (6, 0.012500000160022558), (7, 0.012500000044746468), (8, 0.012500000114562322), (9, 0.012500000221680529), (10, 0.012500000060089181), (11, 0.012500000165634224), (12, 0.012500000424399849), (13, 0.012500000154890683), (14, 0.012500000143454771), (15, 0.012500000119212326), (16, 0.012500000103679128), (17, 0.012500000142243685), (18, 0.012500000440269622), (19, 0.012500000308527669)] 与第5技术:laminated object manufact 相似的主题为:[(0, 0.016666666708534502), (1, 0.01666666692986472), (2, 0.016666666755080724), (3, 0.016666667325515902), (4, 0.68333333051956391), (5, 0.016666666700257116), (6, 0.016666666742171068), (7, 0.016666666690853652), (8, 0.016666666770181162), (9, 0.016666666683445405), (10, 0.016666666821177779), (11, 0.016666666667652999), (12, 0.016666667202277767), (13, 0.016666666699137606), (14, 0.016666666715177164), (15, 0.016666666712675651), (16, 0.016666667197598874), (17, 0.016666666753133944), (18, 0.016666666677748968), (19, 0.016666666727951324)] 与第6技术:selective heat sinter 相似的主题为:[(0, 0.012500000008236487), (1, 0.012500000242189509), (2, 0.012500000115452121), (3, 0.012500000078604575), (4, 0.012500000188981536), (5, 0.76249999500328458), (6, 0.012500000315011571), (7, 0.012500000074153471), (8, 0.012500000191537533), (9, 0.012500000012402453), (10, 0.012500000059782957), (11, 0.012500000040707065), (12, 0.01250000036262164), (13, 0.012500001828313312), (14, 0.012500000630911891), (15, 0.012500000272427156), (16, 0.0125000004143444), (17, 0.012500000089564488), (18, 0.012500000030820422), (19, 0.01250000004065263)] 与第7技术:electron beam melting 相似的主题为:[(0, 0.012500000016454908), (1, 0.012500000050304234), (2, 0.01250000004900541), (3, 0.012500000085850655), (4, 0.012500000002734116), (5, 0.7624999978455782), (6, 0.012500000319303218), (7, 0.012500000007520029), (8, 0.012500000015225669), (9, 0.0125000000032559), (10, 0.012500000191718524), (11, 0.012500000013050492), (12, 0.012500000294125221), (13, 0.012500000033902881), (14, 0.01250000005412431), (15, 0.012500000138971734), (16, 0.012500000171213506), (17, 0.012500000675140826), (18, 0.012500000010269345), (19, 0.012500000022250908)] 与第8技术:selective heat sintering 相似的主题为:[(0, 0.012500000008272661), (1, 0.012500000271256676), (2, 0.012500000116035057), (3, 0.012500000078739632), (4, 0.012500000189553388), (5, 0.76249999323849482), (6, 0.012500000317921743), (7, 0.012500000077258676), (8, 0.012500000194267575), (9, 0.012500000012515692), (10, 0.012500000069095617), (11, 0.012500000055422782), (12, 0.01250000037229872), (13, 0.012500003438939474), (14, 0.012500000632149989), (15, 0.01250000033813994), (16, 0.01250000042327056), (17, 0.01250000009257668), (18, 0.012500000032761164), (19, 0.012500000041029261)] 与第9技术:powder bed and inkjet head 相似的主题为:[(0, 0.010000000010817795), (1, 0.010000000121357063), (2, 0.20932194034723767), (3, 0.010000000005442235), (4, 0.01000000003562615), (5, 0.010000000106448758), (6, 0.010000000075971441), (7, 0.010000000119263049), (8, 0.010000000026851709), (9, 0.010000000146308642), (10, 0.010000000074525062), (11, 0.010000000460679104), (12, 0.37411583750370875), (13, 0.01000000008930217), (14, 0.24656222034213773), (15, 0.010000000152581687), (16, 0.01000000002514225), (17, 0.010000000307013769), (18, 0.010000000016470815), (19, 0.010000000033114326)] 与第10技术:Polymer injection 相似的主题为:[(0, 0.016666666673710518), (1, 0.016666666866131812), (2, 0.016666666839853062), (3, 0.016666666738834195), (4, 0.016666666731599691), (5, 0.016666666668557411), (6, 0.68333333098411175), (7, 0.016666666791825154), (8, 0.016666666711353407), (9, 0.016666666678708759), (10, 0.016666667121143508), (11, 0.016666666797648531), (12, 0.016666666814533489), (13, 0.016666666796876489), (14, 0.016666666889430144), (15, 0.01666666680169827), (16, 0.01666666701933529), (17, 0.016666666699645703), (18, 0.016666666671709539), (19, 0.016666666703293184)] 12]]></content>
      <categories>
        <category>主题模型</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>主题模型</tag>
        <tag>LDA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[利用Bi-LSTM CRF进行命名实体识别]]></title>
    <url>%2F2018%2F04%2F08%2F%E5%88%A9%E7%94%A8Bi-LSTM-CRF%E8%BF%9B%E8%A1%8C%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB%2F</url>
    <content type="text"><![CDATA[0. Introduction 命名实体识别（Named Entity Recognition，NER）是自然语言处理的基础，与分词、词性标注和词块拆分可以说是序列标注的主要任务。 最常用的序列标注模型是线性统计模型，例如隐马、最大熵马尔科夫、条件随机场等。最近 Huang等人提出了基于的Bi-LSTM+CRF的序列标注模型取得了很好的效果，并与其它模型进行了对比，例如： 1234&gt;&gt; I love Paris&gt;&gt; I love Paris O O B-LOC 自己在Huang的论文和代码的基础上，试图将其进行专利文本中的命名实体识别，记录一下这个流程。 1. 数据和任务 命名实体识别（NER）： 是将文本句子分成不同的实体：地名（LOC）、人名（PER）、机构名（ORG）和专有名词（MISC），不是实体的被命名为O。但我们知道实体往往是由多个词组成，因此我们将实体的开始部分用B(Begin)标注，中间词用I标注，这样的标注主要有IOBES。这样一来命名实体识别任务其实就变成了将句子中每个词分类的任务。 CoNLL2003数据集： CoNLL2003的任务是进行命名实体识别，并提供了标注好的数据集，数据规模条数如下： CoNLL2003 traning sentence# token# 14987# 204567# validation sentence# token# 3466# 51578# test sentence# token# 3684# 46666# - label# 9# 数据中每条数据一条句子形式组织，每个词对应一个标注. 123456789EU NNP B-NP B-ORGrejects VBZ B-VP OGerman JJ B-NP B-MISCcall NN I-NP Oto TO B-VP Oboycott VB I-VP OBritish JJ B-NP B-MISClamb NN I-NP O. . O O 可以看出，数据内容中不仅包括了命名实体的标注，还进行了词性标注。所以我们可以利用该数据集进行训练。 2.模型 目前在自然语言处理方面最火的神经网络恐怕非RNN莫属了。在命名实体识别任务中，利用了双向LSTM，在输出层加上了CRF。之所以用CRF替换了softmax，原因在于避免结果中B-ORG、B-ORG顺序出现的情况，而这种情况是不对的。 图1 LSTM 单元 参考来源 图2 LSTM+CRF模型 参考来源 接下来，就详细介绍模型及实现步骤 词嵌入 词嵌入是将每个词转换成向量，转换成低维度稠密的向量为佳，可以自己训练词向量，也可以利用预训练好的，例如GloVe或者Word2Vec，这里采用了GloVe。代码如下： 123456789embeddings = np.zeros([len(vocab), dim]) with open(glove_filename) as f: for line in f: line = line.strip().split(' ') word = line[0] embedding = [float(x) for x in line[1:]] if word in vocab: word_idx = vocab[word] embeddings[word_idx] = np.asarray(embedding) 数据输入 在数据输入时，由于每个句子的长度是不同，因此我们需要将句子的长度都扩充为最长句子的长度。由于是在TensorFlow上实现该模型，需要预先定义两个placeholders，并且从GloVe中找到该词对应的向量，具体代码如下： 12word_ids = tf.placeholder(tf.int32, shape=[None, None],name="word_ids")self.sequence_lengths = tf.placeholder(tf.int32, shape=[None],name="sequence_lengths") 从GloVe中找到该词对应的向量： 12345678910111213141516with tf.variable_scope(&quot;words&quot;): if self.config.embeddings is None: self.logger.info(&quot;WARNING: randomly initializing word vectors&quot;) _word_embeddings = tf.get_variable( name=&quot;_word_embeddings&quot;, dtype=tf.float32, shape=[self.config.nwords, self.config.dim_word]) else: _word_embeddings = tf.Variable( self.config.embeddings, name=&quot;_word_embeddings&quot;, dtype=tf.float32, trainable=self.config.train_embeddings) word_embeddings = tf.nn.embedding_lookup(_word_embeddings, self.word_ids, name=&quot;word_embeddings&quot;) Bi-LSTM层构建 当上述向量构建好以后，就可以构建LSTM模型。在理解模型LSTM模型时确实需要花一些时间，但当用TensorFlow或者Keras构建时，却需要短短几行就可以了，真是太方便太直给了。 12345678with tf.variable_scope("bi-lstm"): cell_fw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_lstm) cell_bw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_lstm) (output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn( cell_fw, cell_bw, self.word_embeddings, sequence_length=self.sequence_lengths, dtype=tf.float32) output = tf.concat([output_fw, output_bw], axis=-1) output = tf.nn.dropout(output, self.dropout) 得出分类得分值 经过模型层的处理，每个词变成了向量，并且都带有各自的特征和上下文。既然结果是9个类别，那么我们完全可以建立一个全连接，大小为 9xk，k为模型隐层的大小。然后得出每个词在每个类上的得分，代码如下： 1234567with tf.variable_scope("proj"): W = tf.get_variable("W", dtype=tf.float32,shape=[2*self.config.hidden_size_lstm, self.config.ntags]) b = tf.get_variable("b", shape=[self.config.ntags],dtype=tf.float32, initializer=tf.zeros_initializer()) nsteps = tf.shape(output)[1] output = tf.reshape(output, [-1, 2*self.config.hidden_size_lstm]) pred = tf.matmul(output, W) + b self.logits = tf.reshape(pred, [-1, nsteps, self.config.ntags]) 利用linear-chain CRF判定类别 在得出分类得分值后，其实这里的得分值是每个词在每个9个类别上的得分。接下来就是要进行命名实体的分割，这里可以有softmax和CRF两种选择。之所以选择CRF，上面也说了一个原因了，softmax其实是不能利用到邻居的分类结果，而linear-chainCRF则能够综合句子的整个得分，示例如下： 图3 linear-chain CRF 参考来源 上图中可以看出linear-chain CRF能够综合整个句子的得分，得分包括了转移概率得分和词分类得分 3.训练 在多分类中，通常把交叉熵损失做损失函数，选择AdamOptimizer作为优化函数。 123456log_likelihood, trans_params = tf.contrib.crf.crf_log_likelihood(self.logits, self.labels, self.sequence_lengths)self.trans_params = trans_paramsself.loss = tf.reduce_mean(-log_likelihood)optimizer = tf.train.AdamOptimizer(self.lr)train_op = optimizer.minimize(self.loss) 训练的效果还是不错的，仅仅训练了3个epoch，F1值就达到了91. 最后几点总结： 专利文本中的命名实体识别，或者说序列标注会更复杂一些。 专利文本的标注数据的训练集不好得到啊。 代码实现也相应放到了github 代码还是需要继续码，论文还是要继续看。 -:) 参考文献 https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html https://github.com/guillaumegenthial/sequence_tagging https://arxiv.org/pdf/1508.01991v1.pdf]]></content>
      <categories>
        <category>自然语言处理</category>
      </categories>
      <tags>
        <tag>NLP</tag>
        <tag>LSTM</tag>
        <tag>NER</tag>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[数据单元包含多条数据怎么办：数据拆分小技巧]]></title>
    <url>%2F2018%2F04%2F07%2F%E6%95%B0%E6%8D%AE%E5%8D%95%E5%85%83%E5%8C%85%E5%90%AB%E5%A4%9A%E6%9D%A1%E6%95%B0%E6%8D%AE%E6%80%8E%E4%B9%88%E5%8A%9E%EF%BC%9A%E6%95%B0%E6%8D%AE%E6%8B%86%E5%88%86%E5%B0%8F%E6%8A%80%E5%B7%A7%2F</url>
    <content type="text"><![CDATA[今天遇到一个数据拆分的问题，觉着有必要记录下来。在做数据处理时，我们会经常遇到一个数据单元格包含多个数据的情况，例如如下的数据: 上图红框中，Licensing_TRANSFEROR列的单元格中含有3值，并且以“；”分割。而在后续的共现分析、网络分析等多种分析则要求我们应该将其拆分成三条数据。这时我们可以尝试用到pandas中的stack、reset_index和join等方法。 具体代码示例如下： 导入pandas 12import pandas as pdfrom pandas import DataFrame,Series 读入数据 12raw_data=pd.read_excel('./2008-2017.xlsx',header=0)raw_data.columns 数据拆分 1raw_tor=raw_data.drop('Licensing_TRANSFEROR',axis=1).join(raw_data['Licensing_TRANSFEROR'].str.split(';',expand=True).stack().reset_index(level=1,drop=True).rename('Licensing_TRANSFEROR')) 其中： stack：该方法可以参考pandas.DataFrame.stack，参数如下： Parameters: level : int, string, or list of these, default last. dropna : boolean, default True.&lt;br&gt; Returns: stacked : DataFrame or Series 该方法可以认为是将数据的一轴旋转堆叠，返回具有层次索引的DataFrame，默认是对最内层轴，返回结果自动排序，官方给出的例子如下： 12345678910&gt;&gt;&gt;s a bone 1. 2.two 3. 4.&gt;&gt;&gt;s.stack()one a 1 b 2two a 3 b 4 reset_index：该方法可以参考pandas.DataFrame.reset_index，参数如下： 该方法可以实现对指定的列的重新索引，官方例子如下： Parameters:&lt;br&gt; level : int, str, tuple, or list, default None. drop : boolean, default False. inplace : boolean, default False. col_level : int or str, default 0. col_fill : object, default . &lt;br&gt; Returns: &lt;br&gt; resetted : DataFrame 该方法可以实现对指定的列的重新索引，官方例子如下： 1234567891011121314151617181920&gt;&gt;&gt;df = pd.DataFrame([('bird', 389.0),... ('bird', 24.0),... ('mammal', 80.5),... ('mammal', np.nan)],... index=['falcon', 'parrot', 'lion', 'monkey'],... columns=('class', 'max_speed'))&gt;&gt;&gt;df class max_speedfalcon bird 389.0parrot bird 24.0lion mammal 80.5monkey mammal NaN&gt;&gt;&gt;df.reset_index(drop=True) class max_speed0 bird 389.01 bird 24.02 mammal 80.53 mammal NaN 数据导出 1raw_tor.to_excel('./2008-2017_new.xlsx') 处理结果如下： 最后，数据拆分还是能用到很多分析之中，具体到上面这个案例是在分析专利许可数据。后期的让与人和受让人之间的专利许可网络分析、专利权人社会网络分析都要首先进行该处理。 参考文献 https://www.cnblogs.com/lemonbit/p/7270045.html&lt;br&gt; http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reset_index.html&lt;br&gt; http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.stack.html]]></content>
      <categories>
        <category>数据处理</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>数据处理</tag>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[博客搬家啦~]]></title>
    <url>%2F2018%2F04%2F06%2F%E5%8D%9A%E5%AE%A2%E6%90%AC%E5%AE%B6%E5%95%A6%2F</url>
    <content type="text"><![CDATA[博客搬家啦 今天开始，博客正式搬家了，来到了github.io，这一圣地。 我的github地址：ChaoOnGithub。 这里算是自己的技术随想、生活感悟和三言两语的精神家园!]]></content>
      <categories>
        <category>life</category>
      </categories>
      <tags>
        <tag>博客</tag>
        <tag>生活</tag>
      </tags>
  </entry>
</search>
