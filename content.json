{"pages":[{"title":"","text":"","path":"404.html"},{"title":"categories","text":"","path":"categories/index.html"},{"title":"about","text":"","path":"about/index.html"},{"title":"tags","text":"","path":"tags/index.html"}],"posts":[{"title":"利用Bi-LSTM CRF进行命名实体识别","text":"0. Introduction 命名实体识别（Named Entity Recognition，NER）是自然语言处理的基础，与分词、词性标注和词块拆分可以说是序列标注的主要任务。 最常用的序列标注模型是线性统计模型，例如隐马、最大熵马尔科夫、条件随机场等。最近 Huang等人提出了基于的Bi-LSTM+CRF的序列标注模型取得了很好的效果，并与其它模型进行了对比，例如： 1234&gt;&gt; I love Paris&gt;&gt; I love Paris O O B-LOC 自己在Huang的论文和代码的基础上，试图将其进行专利文本中的命名实体识别，记录一下这个流程。 1. 数据和任务 命名实体识别（NER）： 是将文本句子分成不同的实体：地名（LOC）、人名（PER）、机构名（ORG）和专有名词（MISC），不是实体的被命名为O。但我们知道实体往往是由多个词组成，因此我们将实体的开始部分用B(Begin)标注，中间词用I标注，这样的标注主要有IOBES。这样一来命名实体识别任务其实就变成了将句子中每个词分类的任务。 CoNLL2003数据集： CoNLL2003的任务是进行命名实体识别，并提供了标注好的数据集，数据规模条数如下： CoNLL2003 traning sentence# token# 14987# 204567# validation sentence# token# 3466# 51578# test sentence# token# 3684# 46666# - label# 9# 数据中每条数据一条句子形式组织，每个词对应一个标注. 123456789EU NNP B-NP B-ORGrejects VBZ B-VP OGerman JJ B-NP B-MISCcall NN I-NP Oto TO B-VP Oboycott VB I-VP OBritish JJ B-NP B-MISClamb NN I-NP O. . O O 可以看出，数据内容中不仅包括了命名实体的标注，还进行了词性标注。所以我们可以利用该数据集进行训练。 2.模型 目前在自然语言处理方面最火的神经网络恐怕非RNN莫属了。在命名实体识别任务中，利用了双向LSTM，在输出层加上了CRF。之所以用CRF替换了softmax，原因在于避免结果中B-ORG、B-ORG顺序出现的情况，而这种情况是不对的。 图1 LSTM 单元 参考来源 图2 LSTM+CRF模型 参考来源 接下来，就详细介绍模型及实现步骤 词嵌入 词嵌入是将每个词转换成向量，转换成低维度稠密的向量为佳，可以自己训练词向量，也可以利用预训练好的，例如GloVe或者Word2Vec，这里采用了GloVe。代码如下： 123456789embeddings = np.zeros([len(vocab), dim]) with open(glove_filename) as f: for line in f: line = line.strip().split(' ') word = line[0] embedding = [float(x) for x in line[1:]] if word in vocab: word_idx = vocab[word] embeddings[word_idx] = np.asarray(embedding) 数据输入 在数据输入时，由于每个句子的长度是不同，因此我们需要将句子的长度都扩充为最长句子的长度。由于是在TensorFlow上实现该模型，需要预先定义两个placeholders，并且从GloVe中找到该词对应的向量，具体代码如下： 12word_ids = tf.placeholder(tf.int32, shape=[None, None],name=\"word_ids\")self.sequence_lengths = tf.placeholder(tf.int32, shape=[None],name=\"sequence_lengths\") 从GloVe中找到该词对应的向量： 12345678910111213141516with tf.variable_scope(&quot;words&quot;): if self.config.embeddings is None: self.logger.info(&quot;WARNING: randomly initializing word vectors&quot;) _word_embeddings = tf.get_variable( name=&quot;_word_embeddings&quot;, dtype=tf.float32, shape=[self.config.nwords, self.config.dim_word]) else: _word_embeddings = tf.Variable( self.config.embeddings, name=&quot;_word_embeddings&quot;, dtype=tf.float32, trainable=self.config.train_embeddings) word_embeddings = tf.nn.embedding_lookup(_word_embeddings, self.word_ids, name=&quot;word_embeddings&quot;) Bi-LSTM层构建 当上述向量构建好以后，就可以构建LSTM模型。在理解模型LSTM模型时确实需要花一些时间，但当用TensorFlow或者Keras构建时，却需要短短几行就可以了，真是太方便太直给了。 12345678with tf.variable_scope(\"bi-lstm\"): cell_fw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_lstm) cell_bw = tf.contrib.rnn.LSTMCell(self.config.hidden_size_lstm) (output_fw, output_bw), _ = tf.nn.bidirectional_dynamic_rnn( cell_fw, cell_bw, self.word_embeddings, sequence_length=self.sequence_lengths, dtype=tf.float32) output = tf.concat([output_fw, output_bw], axis=-1) output = tf.nn.dropout(output, self.dropout) 得出分类得分值 经过模型层的处理，每个词变成了向量，并且都带有各自的特征和上下文。既然结果是9个类别，那么我们完全可以建立一个全连接，大小为 9xk，k为模型隐层的大小。然后得出每个词在每个类上的得分，代码如下： 1234567with tf.variable_scope(\"proj\"): W = tf.get_variable(\"W\", dtype=tf.float32,shape=[2*self.config.hidden_size_lstm, self.config.ntags]) b = tf.get_variable(\"b\", shape=[self.config.ntags],dtype=tf.float32, initializer=tf.zeros_initializer()) nsteps = tf.shape(output)[1] output = tf.reshape(output, [-1, 2*self.config.hidden_size_lstm]) pred = tf.matmul(output, W) + b self.logits = tf.reshape(pred, [-1, nsteps, self.config.ntags]) 利用linear-chain CRF判定类别 在得出分类得分值后，其实这里的得分值是每个词在每个9个类别上的得分。接下来就是要进行命名实体的分割，这里可以有softmax和CRF两种选择。之所以选择CRF，上面也说了一个原因了，softmax其实是不能利用到邻居的分类结果，而linear-chainCRF则能够综合句子的整个得分，示例如下： 图3 linear-chain CRF 参考来源 上图中可以看出linear-chain CRF能够综合整个句子的得分，得分包括了转移概率得分和词分类得分 3.训练 在多分类中，通常把交叉熵损失做损失函数，选择AdamOptimizer作为优化函数。 123456log_likelihood, trans_params = tf.contrib.crf.crf_log_likelihood(self.logits, self.labels, self.sequence_lengths)self.trans_params = trans_paramsself.loss = tf.reduce_mean(-log_likelihood)optimizer = tf.train.AdamOptimizer(self.lr)train_op = optimizer.minimize(self.loss) 训练的效果还是不错的，仅仅训练了3个epoch，F1值就达到了91. 最后几点总结： 专利文本中的命名实体识别，或者说序列标注会更复杂一些。 专利文本的标注数据的训练集不好得到啊。 代码实现也相应放到了github 代码还是需要继续码，论文还是要继续看。 -:) 参考文献 https://guillaumegenthial.github.io/sequence-tagging-with-tensorflow.html https://github.com/guillaumegenthial/sequence_tagging https://arxiv.org/pdf/1508.01991v1.pdf","path":"2018/04/08/利用Bi-LSTM-CRF进行命名实体识别/"},{"title":"数据单元包含多条数据怎么办：数据拆分小技巧","text":"今天遇到一个数据拆分的问题，觉着有必要记录下来。在做数据处理时，我们会经常遇到一个数据单元格包含多个数据的情况，例如如下的数据: 上图红框中，Licensing_TRANSFEROR列的单元格中含有3值，并且以“；”分割。而在后续的共现分析、网络分析等多种分析则要求我们应该将其拆分成三条数据。这时我们可以尝试用到pandas中的stack、reset_index和join等方法。 具体代码示例如下： 导入pandas 12import pandas as pdfrom pandas import DataFrame,Series 读入数据 12raw_data=pd.read_excel('./2008-2017.xlsx',header=0)raw_data.columns 数据拆分 1raw_tor=raw_data.drop('Licensing_TRANSFEROR',axis=1).join(raw_data['Licensing_TRANSFEROR'].str.split(';',expand=True).stack().reset_index(level=1,drop=True).rename('Licensing_TRANSFEROR')) 其中： stack：该方法可以参考pandas.DataFrame.stack，参数如下： Parameters: level : int, string, or list of these, default last. dropna : boolean, default True.&lt;br&gt; Returns: stacked : DataFrame or Series 该方法可以认为是将数据的一轴旋转堆叠，返回具有层次索引的DataFrame，默认是对最内层轴，返回结果自动排序，官方给出的例子如下： 12345678910&gt;&gt;&gt;s a bone 1. 2.two 3. 4.&gt;&gt;&gt;s.stack()one a 1 b 2two a 3 b 4 reset_index：该方法可以参考pandas.DataFrame.reset_index，参数如下： 该方法可以实现对指定的列的重新索引，官方例子如下： Parameters:&lt;br&gt; level : int, str, tuple, or list, default None. drop : boolean, default False. inplace : boolean, default False. col_level : int or str, default 0. col_fill : object, default . &lt;br&gt; Returns: &lt;br&gt; resetted : DataFrame 该方法可以实现对指定的列的重新索引，官方例子如下： 1234567891011121314151617181920&gt;&gt;&gt;df = pd.DataFrame([('bird', 389.0),... ('bird', 24.0),... ('mammal', 80.5),... ('mammal', np.nan)],... index=['falcon', 'parrot', 'lion', 'monkey'],... columns=('class', 'max_speed'))&gt;&gt;&gt;df class max_speedfalcon bird 389.0parrot bird 24.0lion mammal 80.5monkey mammal NaN&gt;&gt;&gt;df.reset_index(drop=True) class max_speed0 bird 389.01 bird 24.02 mammal 80.53 mammal NaN 数据导出 1raw_tor.to_excel('./2008-2017_new.xlsx') 处理结果如下： 最后，数据拆分还是能用到很多分析之中，具体到上面这个案例是在分析专利许可数据。后期的让与人和受让人之间的专利许可网络分析、专利权人社会网络分析都要首先进行该处理。 参考文献 https://www.cnblogs.com/lemonbit/p/7270045.html&lt;br&gt; http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.reset_index.html&lt;br&gt; http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.stack.html","path":"2018/04/07/数据单元包含多条数据怎么办：数据拆分小技巧/"},{"title":"博客搬家啦~","text":"博客搬家啦 今天开始，博客正式搬家了，来到了github.io，这一圣地。 我的github地址：ChaoOnGithub。 这里算是自己的技术随想、生活感悟和三言两语的精神家园!","path":"2018/04/06/博客搬家啦/"}]}